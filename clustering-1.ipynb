{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Mengxia Shi & Mikhail Zakharov | 5 SDBD B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "from scipy.io import arff\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selected examples:\n",
    "# dataset (true number of clusters; ?params)\n",
    "# k-means (good): DS-577 (3), 2d-4c (4), R15 (15), spherical_5_2 (5)\n",
    "# k-means (bad): 3-spiral (3), banana (2), curves1 vs. curves2 (2)\n",
    "# agglo (good): curves2 (2;single), donut1 (2;single), DS-577 (3;complete/ward), ds4c2sc8 (?)\n",
    "# agglo (bad): flame (2), zelnik2 (3), disk-4500n (2)\n",
    "# dbscan (good): spiral (3;0.5,5), donut3 (3;0.02,5), dartboard2 (4;0.01,5)\n",
    "# dbscan (bad): 2d-3c-no123 (3), disk-4600n (2), ds4c2sc8 (8)\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    data_load = arff.loadarff(open(data_path + dataset + '.arff', 'r'))[0]\n",
    "    x = [point[0] for point in data_load]\n",
    "    y = [point[1] for point in data_load]\n",
    "    data_xy = list(zip(x,y))\n",
    "    return x, y, data_xy\n",
    "\n",
    "dataset = 'DS-577'\n",
    "true_cluster_number = 3\n",
    "\n",
    "data_path = \"clustering-benchmark-master/src/main/resources/datasets/artificial/\"\n",
    "\n",
    "x, y, data_xy = load_dataset(dataset)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.title(\"Initial data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Clustering with known true number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=true_cluster_number, init='k-means++').fit(data_xy)\n",
    "\n",
    "center_x = [point[0] for point in kmeans.cluster_centers_]\n",
    "center_y = [point[1] for point in kmeans.cluster_centers_]\n",
    "\n",
    "plt.scatter(x, y, c=kmeans.labels_, cmap='rainbow')\n",
    "plt.scatter(center_x, center_y, marker=\"x\", c=\"000000\")\n",
    "plt.title(\"Data after clustering with k-means\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD\n",
    "for dataset in [['DS-577', 3], ['2d-4c', 4], ['R15', 15], ['spherical_5_2', 5]]:\n",
    "    x2, y2, data_xy2 = load_dataset(dataset[0])\n",
    "    kmeans = cluster.KMeans(n_clusters=dataset[1], init='k-means++').fit(data_xy2)\n",
    "    plt.scatter(x2, y2, c=kmeans.labels_, cmap='rainbow')\n",
    "    plt.title(\"Data after KMEANS clustering\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Automatically determining the true number of clusters using evaluation metrics\n",
    "\n",
    "Many of the evaluation metrics provided by `sklearn.metrics` require knowledge of the ground truth. They are therefore academic in nature and shall not be considered, as in reality there is no known ground truth.\n",
    "\n",
    "The metrics considered are therefore the:  \n",
    "* i) Silhouette Coefficient,\n",
    "* ii) Calinski-Harabasz Index, and\n",
    "* iii) Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2.i Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = -1 # Lower bound for Silhouette coefficient (higher is better)\n",
    "score = 0      # Initial score\n",
    "n_clust = 2    # There must be at least two clusters \n",
    "\n",
    "results_SC = []\n",
    "times_SC = []\n",
    "\n",
    "while n_clust < 20:\n",
    "    time_start = time.time()\n",
    "    \n",
    "    kmeans = cluster.KMeans(n_clusters=n_clust, init='k-means++').fit(data_xy)\n",
    "    \n",
    "    time_post_cluster = time.time()\n",
    "    \n",
    "    score = metrics.silhouette_score(data_xy, kmeans.labels_)\n",
    "    \n",
    "    time_post_score = time.time()\n",
    "    \n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "    \n",
    "    results_SC.append((n_clust, score))\n",
    "    times_SC.append((time_start, time_post_cluster, time_post_score))\n",
    "    \n",
    "    n_clust += 1\n",
    "\n",
    "best_n_clusters_guess = max(results_SC, key=lambda r: r[1])\n",
    "print(f\"I determine the number of clusters to be: {best_n_clusters_guess[0]}.\")\n",
    "print(f\"It took me {times_SC[-1][2]-times_SC[0][0]} seconds to figure that out. ({len(results_SC)} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2.ii Calinski-Harabasz index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = -1 # Assumed lower bound for CH index (higher is better)\n",
    "score = 0      # Initial score\n",
    "n_clust = 2    # There must be at least two clusters \n",
    "\n",
    "results_CHI = []\n",
    "times_CHI = []\n",
    "\n",
    "while n_clust < 20:\n",
    "    time_start = time.time()\n",
    "    \n",
    "    kmeans = cluster.KMeans(n_clusters=n_clust, init='k-means++').fit(data_xy)\n",
    "    \n",
    "    time_post_cluster = time.time()\n",
    "    \n",
    "    score = metrics.calinski_harabasz_score(data_xy, kmeans.labels_)\n",
    "    \n",
    "    time_post_score = time.time()\n",
    "    \n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "    \n",
    "    results_CHI.append((n_clust, score))\n",
    "    times_CHI.append((time_start, time_post_cluster, time_post_score))\n",
    "    \n",
    "    n_clust += 1\n",
    "\n",
    "best_n_clusters_guess = max(results_CHI, key=lambda r: r[1])\n",
    "print(f\"I determine the number of clusters to be: {best_n_clusters_guess[0]}.\")\n",
    "print(f\"It took me {times_CHI[-1][2]-times_CHI[0][0]} seconds to figure that out. ({len(results_CHI)} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2.iii Daviesâ€“Bouldin index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = 1 # Assumed upper bound for DB index (lower is better)\n",
    "score = 1.1   # Initial score\n",
    "n_clust = 2   # There must be at least two clusters \n",
    "\n",
    "results_DBI = []\n",
    "times_DBI = []\n",
    "\n",
    "while n_clust < 20:\n",
    "    time_start = time.time()\n",
    "    \n",
    "    kmeans = cluster.KMeans(n_clusters=n_clust, init='k-means++').fit(data_xy)\n",
    "    \n",
    "    time_post_cluster = time.time()\n",
    "    \n",
    "    score = metrics.davies_bouldin_score(data_xy, kmeans.labels_)\n",
    "    \n",
    "    time_post_score = time.time()\n",
    "    \n",
    "    if score < min_score:\n",
    "        min_score = score\n",
    "    \n",
    "    results_DBI.append((n_clust, score))\n",
    "    times_DBI.append((time_start, time_post_cluster, time_post_score))\n",
    "    \n",
    "    n_clust += 1\n",
    "\n",
    "best_n_clusters_guess = min(results_DBI, key=lambda r: r[1])\n",
    "print(f\"I determine the number of clusters to be: {best_n_clusters_guess[0]}.\")\n",
    "print(f\"It took me {times_DBI[-1][2]-times_DBI[0][0]} seconds to figure that out. ({len(results_DBI)} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Limits of k-means clustering\n",
    "\n",
    "(maybe the three metrics above give a good reading for a bad clustering, in which case another metric is needed to judge the clustering's \"goodness\" ~ test and develop further)\n",
    "\n",
    "(use curves1 vs. curves2 example to explain the center and convexity limitation of k-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD\n",
    "for dataset in [['3-spiral', 3], ['banana', 2], ['curves1', 2]]:\n",
    "    x2, y2, data_xy2 = load_dataset(dataset[0])\n",
    "    kmeans = cluster.KMeans(n_clusters=dataset[1], init='k-means++').fit(data_xy2)\n",
    "    plt.scatter(x2, y2, c=kmeans.labels_, cmap='rainbow')\n",
    "    plt.title(\"Data after KMEANS clustering\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Clustering with known true number of clusters and with different linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "agglo = cluster.AgglomerativeClustering(n_clusters=None, linkage=\"ward\", distance_threshold=5).fit(data_xy)\n",
    "    \n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(agglo, truncate_mode='level', p=10)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD\n",
    "for dataset in [['curves2', 2, 'single'], ['donut1', 2, 'single'], ['DS-577', 3, 'complete']]:\n",
    "    x2, y2, data_xy2 = load_dataset(dataset[0])\n",
    "    agglo = cluster.AgglomerativeClustering(n_clusters=dataset[1], linkage=dataset[2]).fit(data_xy2)\n",
    "    plt.scatter(x2, y2, c=agglo.labels_, cmap='rainbow')\n",
    "    plt.title(\"Data after Agglomerative Clustering clustering\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dataset in [['R15',15], ['smile3',4], ['diamond9',9], ['2d-3c-no123', 3]]:\n",
    "    x2, y2, data_xy2 = load_dataset(dataset[0])\n",
    "    for linkage in ['ward', 'complete', 'average', 'single']:\n",
    "        agglo = cluster.AgglomerativeClustering(n_clusters=dataset[1], linkage=linkage).fit(data_xy2)\n",
    "        plt.scatter(x2, y2, c=agglo.labels_, cmap='rainbow')\n",
    "        plt.title(\"Data after clustering with agglomerative clustering (linkage=\" + linkage + \")\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Automatically determining the true number of clusters using evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2.i Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = -1 # Lower bound for Silhouette coefficient (higher is better)\n",
    "score = 0      # Initial score\n",
    "n_clust = 2    # There must be at least two clusters \n",
    "\n",
    "results_SC = []\n",
    "times_SC = []\n",
    "\n",
    "# or just do 2-30 or something\n",
    "#while score > (max_score * 0.9):\n",
    "while n_clust < 10:\n",
    "    time_start = time.time()\n",
    "    \n",
    "    agglo = cluster.AgglomerativeClustering(n_clusters=n_clust, linkage='ward').fit(data_xy)\n",
    "    \n",
    "    time_post_cluster = time.time()\n",
    "    \n",
    "    score = metrics.silhouette_score(data_xy, agglo.labels_)\n",
    "    \n",
    "    time_post_score = time.time()\n",
    "    \n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "    \n",
    "    results_SC.append((n_clust, score))\n",
    "    times_SC.append((time_start, time_post_cluster, time_post_score))\n",
    "    \n",
    "    n_clust += 1\n",
    "\n",
    "best_n_clusters_guess = max(results_SC, key=lambda r: r[1])\n",
    "print(f\"I determine the number of clusters to be: {best_n_clusters_guess[0]}.\")\n",
    "print(f\"It took me {times_SC[-1][2]-times_SC[0][0]} seconds to figure that out. ({len(results_SC)} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2.ii Calinski-Harabasz index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = -1 # Assumed lower bound for CH index (higher is better)\n",
    "score = 0      # Initial score\n",
    "n_clust = 2    # There must be at least two clusters \n",
    "\n",
    "results_CHI = []\n",
    "times_CHI = []\n",
    "\n",
    "# or just do 2-30 or something\n",
    "#while score > (max_score * 0.9):\n",
    "while n_clust < 10:\n",
    "    time_start = time.time()\n",
    "    \n",
    "    agglo = cluster.AgglomerativeClustering(n_clusters=n_clust, linkage='ward').fit(data_xy)\n",
    "        \n",
    "    time_post_cluster = time.time()\n",
    "    \n",
    "    score = metrics.calinski_harabasz_score(data_xy, agglo.labels_)\n",
    "    \n",
    "    time_post_score = time.time()\n",
    "    \n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "    \n",
    "    results_CHI.append((n_clust, score))\n",
    "    times_CHI.append((time_start, time_post_cluster, time_post_score))\n",
    "    \n",
    "    n_clust += 1\n",
    "\n",
    "best_n_clusters_guess = max(results_CHI, key=lambda r: r[1])\n",
    "print(f\"I determine the number of clusters to be: {best_n_clusters_guess[0]}.\")\n",
    "print(f\"It took me {times_CHI[-1][2]-times_CHI[0][0]} seconds to figure that out. ({len(results_CHI)} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2.iii Daviesâ€“Bouldin index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = 1 # Assumed upper bound for DB index (lower is better)\n",
    "score = 1.1   # Initial score\n",
    "n_clust = 2   # There must be at least two clusters \n",
    "\n",
    "results_DBI = []\n",
    "times_DBI = []\n",
    "\n",
    "# or just do 2-30 or something\n",
    "# while score < (min_score * 1.423):\n",
    "while n_clust < 10:\n",
    "    time_start = time.time()\n",
    "    \n",
    "    agglo = cluster.AgglomerativeClustering(n_clusters=n_clust, linkage='ward').fit(data_xy)\n",
    "    \n",
    "    time_post_cluster = time.time()\n",
    "    \n",
    "    score = metrics.davies_bouldin_score(data_xy, agglo.labels_)\n",
    "    \n",
    "    time_post_score = time.time()\n",
    "    \n",
    "    if score < min_score:\n",
    "        min_score = score\n",
    "    \n",
    "    results_DBI.append((n_clust, score))\n",
    "    times_DBI.append((time_start, time_post_cluster, time_post_score))\n",
    "    \n",
    "    n_clust += 1\n",
    "\n",
    "best_n_clusters_guess = min(results_DBI, key=lambda r: r[1])\n",
    "print(f\"I determine the number of clusters to be: {best_n_clusters_guess[0]}.\")\n",
    "print(f\"It took me {times_DBI[-1][2]-times_DBI[0][0]} seconds to figure that out. ({len(results_DBI)} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Limits of agglomerative clustering\n",
    "\n",
    "(low-separation, uniform density)\n",
    "\n",
    "(only based on distance between points, explain with zelnik example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD\n",
    "for dataset in [['flame', 2, 'single'], ['zelnik2', 3, 'single'], ['disk-4500n', 2, 'complete']]:\n",
    "    x2, y2, data_xy2 = load_dataset(dataset[0])\n",
    "    agglo = cluster.AgglomerativeClustering(n_clusters=dataset[1], linkage=dataset[2]).fit(data_xy2)\n",
    "    plt.scatter(x2, y2, c=agglo.labels_, cmap='rainbow')\n",
    "    plt.title(\"Data after Agglomerative Clustering clustering\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Clustering with manually set `eps` and `min_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbscan = cluster.DBSCAN(eps=0.06, min_samples=5).fit(data_xy)\n",
    "\n",
    "plt.scatter(x, y, c=dbscan.labels_, cmap='rainbow')\n",
    "plt.title(\"Data after DBSCAN clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD\n",
    "for dataset in [['spiral', 0.5, 3], ['donut3', 0.02, 5], ['dartboard2', 0.01, 5]]:\n",
    "    x2, y2, data_xy2 = load_dataset(dataset[0])\n",
    "    dbscan = cluster.DBSCAN(eps=dataset[1], min_samples=dataset[2]).fit(data_xy2)\n",
    "    plt.scatter(x2, y2, c=dbscan.labels_, cmap='rainbow')\n",
    "    plt.title(\"Data after DBSCAN clustering\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Automatically determining the true number of clusters using evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_eps(data):\n",
    "    \"\"\"\n",
    "    Determine a suitable value for eps for use with DBSCAN\n",
    "    \n",
    "    Args:\n",
    "        data: zipped list containing data points, like [(x1,y1), (x2,y2), ...]\n",
    "    \n",
    "    Returns:\n",
    "        An eps value greater than the distance separating 85% of points from each other.\n",
    "    \"\"\"\n",
    "    tree = neighbors.KDTree(data)\n",
    "    distances = [] # average distance to 5 closest points\n",
    "    \n",
    "    for (i, point) in enumerate(data):\n",
    "        dist, _ = tree.query(data[i:i+1], k=5+1) # k=1 compares distance to itself\n",
    "        distances.append(np.mean(dist[0][1:]))\n",
    "    \n",
    "    \"\"\" Optional historgram for sanity check \"\"\"\n",
    "    # plt.hist(distances, bins=50)\n",
    "    # plt.show()\n",
    "\n",
    "    \"\"\" Optional scatter plot for sanity check \"\"\"\n",
    "    # plt.scatter(np.linspace(1,len(distances),len(distances)), sorted(distances), s=1)\n",
    "    # plt.show()\n",
    "    \n",
    "    p85 = int(np.floor(len(distances) * 0.85))\n",
    "    \n",
    "    # Sorting distances effectively gives the cumulative distribution \"function\".\n",
    "    return sorted(distances)[p85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get approximative best eps\n",
    "def approx_eps(data_xy, k):\n",
    "    sum_dis = 0\n",
    "    sample_size = 50\n",
    "    for i0 in range(sample_size):\n",
    "        i1 = random.randint(0, len(data_xy) - 1)\n",
    "        min_dis = []\n",
    "        for i2 in range(0, len(data_xy)):\n",
    "            if i1 == i2:\n",
    "                continue\n",
    "            dis = math.sqrt((data_xy[i1][0] - data_xy[i2][0]) ** 2 + (data_xy[i1][1] - data_xy[i2][1]) ** 2)\n",
    "            for i3 in range(k):\n",
    "                if len(min_dis) < k:\n",
    "                    min_dis.append(dis)\n",
    "                    if len(min_dis) == k:\n",
    "                        min_dis.sort()\n",
    "                    break\n",
    "                elif dis < min_dis[i3]:\n",
    "                    min_dis[i3] = dis\n",
    "                    break\n",
    "        sum_dis += min_dis[k - 1]\n",
    "    return round(sum_dis / sample_size, 4)\n",
    "\n",
    "# get number of clusters without noise\n",
    "def filter_noise(clustering, min_cluster_size=50):\n",
    "    cluster_count = [0] * len(set(clustering))\n",
    "    for i in clustering:\n",
    "        if i != -1:\n",
    "            cluster_count[i] += 1\n",
    "    size = 0\n",
    "    for i in range(len(cluster_count)):\n",
    "        if cluster_count[i] >= min_cluster_size:\n",
    "            size += 1\n",
    "    return size\n",
    "\n",
    "for dataset in ['spiral', 'donut3', 'dartboard2', '2d-3c-no123', 'disk-4600n', 'ds4c2sc8']:\n",
    "    x2, y2, data_xy2 = load_dataset(dataset)\n",
    "    min_samples = 2\n",
    "    best = -1\n",
    "    best_clustering = []\n",
    "    best_min_samples = -1\n",
    "    best_eps = -1\n",
    "    start = time.time()\n",
    "    while min_samples <= 15:\n",
    "        eps = approx_eps(data_xy2, min_samples)\n",
    "        clustering = cluster.DBSCAN(eps=eps, min_samples=min_samples).fit_predict(data_xy2)\n",
    "        if len(set(clustering)) == 1:\n",
    "            min_samples += 1\n",
    "            continue\n",
    "        score = metrics.silhouette_score(data_xy2, clustering, metric='euclidean')\n",
    "        if score > best:\n",
    "            best = score\n",
    "            best_clustering = clustering\n",
    "            best_min_samples = min_samples\n",
    "            best_eps = eps\n",
    "        min_samples += 1\n",
    "    print(f\"For dataset '{dataset}':\")\n",
    "    print(f\"I determine the number of clusters to be: {filter_noise(best_clustering)}.\")\n",
    "    print(f\"It took me {time.time() - start} seconds to figure that out.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Limits of DBSCAN clustering\n",
    "\n",
    "struggles with uniform density\n",
    "\n",
    "and can get tripped up on \"intermediary chains\" (see cure-* examples)\n",
    "\n",
    "but does well against noise (duh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD\n",
    "for dataset in [['2d-3c-no123', 0.02, 3], ['disk-4600n', 0.5, 3], ['ds4c2sc8', 0.02, 2]]:\n",
    "    x2, y2, data_xy2 = load_dataset(dataset[0])\n",
    "    dbscan = cluster.DBSCAN(eps=dataset[1], min_samples=dataset[2]).fit(data_xy2)\n",
    "    plt.scatter(x2, y2, c=dbscan.labels_, cmap='rainbow')\n",
    "    plt.title(\"Data after DBSCAN clustering\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"insensitive to density variation\"\n",
    "\n",
    "compare DBSCAN and HDBSCAN -- pros cons of each, performance (time) difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Clustering with manually set `min_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maa = hdbscan.HDBSCAN(min_cluster_size=10).fit(data_xy)\n",
    "\n",
    "plt.scatter(x, y, c=maa.labels_, cmap='rainbow')\n",
    "plt.title(\"Data after HDBSCAN clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD\n",
    "for dataset in [['spiral', 5], ['donut3', 5], ['dartboard2', 3]]:\n",
    "    x2, y2, data_xy2 = load_dataset(dataset[0])\n",
    "    clustering = hdbscan(data_xy2, min_samples=dataset[1], min_cluster_size=50)\n",
    "    plt.scatter(x2, y2, c=clustering[0], cmap='rainbow')\n",
    "    plt.title(\"Data after HDBSCAN clustering\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Automatically determining the true number of clusters using evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['spiral', 'donut3', 'dartboard2', '2d-3c-no123', 'disk-4600n', 'ds4c2sc8']:\n",
    "    x2, y2, data_xy2 = load_dataset(dataset)\n",
    "    min_samples = 2\n",
    "    best = -1\n",
    "    best_clustering = []\n",
    "    best_min_samples = -1\n",
    "    start = time.time()\n",
    "    while min_samples <= 15:\n",
    "        clustering = hdbscan(data_xy2, min_samples=min_samples, min_cluster_size=50)[0]\n",
    "        if len(set(clustering)) == 1:\n",
    "            min_samples += 1\n",
    "            continue\n",
    "        score = metrics.silhouette_score(data_xy2, clustering, metric='euclidean')\n",
    "        if score > best:\n",
    "            best = score\n",
    "            best_clustering = clustering\n",
    "            best_min_samples = min_samples\n",
    "        min_samples += 1\n",
    "    print(f\"For dataset '{dataset}':\")\n",
    "    print(f\"I determine the number of clusters to be: {len(set(best_clustering))}.\")\n",
    "    print(f\"It took me {time.time() - start} seconds to figure that out.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Limits of HDBSCAN clustering\n",
    "\n",
    "struggles with uniform density\n",
    "\n",
    "and can get tripped up on \"intermediary chains\" (see cure-* examples)\n",
    "\n",
    "but does well against noise (duh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD\n",
    "for dataset in [['2d-3c-no123', 7], ['disk-4600n', 2], ['ds4c2sc8', 3]]:\n",
    "    x2, y2, data_xy2 = load_dataset(dataset[0])\n",
    "    clustering = hdbscan(data_xy2, min_samples=dataset[1], min_cluster_size=50)\n",
    "    plt.scatter(x2, y2, c=clustering[0], cmap='rainbow')\n",
    "    plt.title(\"Data after HDBSCAN clustering\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "Measure time:\n",
    "* to compare between different clustering methods\n",
    "* to see how execution time depends on number of clusters to calculate\n",
    "* between different stop conditions\\*\n",
    "\n",
    "\\* All these things are choices to justify. What stop condition to use (time vs. accuracy trade-off)? ...\n",
    "\n",
    "evaluate method within itself (diff params) and with other methods using the best config on same data\n",
    "\n",
    "do all three metrics, because you can then decide on a consensus between the three\n",
    "\n",
    "maybe try something with dendrograms?\n",
    "\n",
    "maybe don't be dumb and use _convex_ datasets for DBSCAN so you can also evaluate using the same metrics\n",
    "\n",
    "## Links\n",
    "* https://pymfe.readthedocs.io/en/latest/auto_examples/03_miscellaneous_examples/plot_using_pandas_csv_arff.html\n",
    "\n",
    "* https://github.com/deric/clustering-benchmark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
